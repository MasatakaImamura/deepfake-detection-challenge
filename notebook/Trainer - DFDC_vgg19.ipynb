{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data_dir = '..\\\\input'\n",
    "\n",
    "metadata_path = glob.glob(os.path.join(data_dir, '*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata\n",
    "metadata = pd.DataFrame()\n",
    "for path in metadata_path:\n",
    "    metadata = pd.concat([metadata, pd.read_json(path).T], axis=0)\n",
    "    \n",
    "metadata.reset_index(inplace=True)\n",
    "metadata = metadata.rename(columns={'index': 'mov'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mov</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>owxbbpjpch.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>wynotylpnm.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vpmyeepbep.mp4</td>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fzvpbrzssi.mp4</td>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>htorvhbcae.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>wclvkepakb.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fckxaqjbxk.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>vpmyeepbep.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mov label  split        original\n",
       "0  owxbbpjpch.mp4  FAKE  train  wynotylpnm.mp4\n",
       "1  vpmyeepbep.mp4  REAL  train             NaN\n",
       "2  fzvpbrzssi.mp4  REAL  train             NaN\n",
       "3  htorvhbcae.mp4  FAKE  train  wclvkepakb.mp4\n",
       "4  fckxaqjbxk.mp4  FAKE  train  vpmyeepbep.mp4"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FAKE    5593\n",
       "REAL     643\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label\n",
    "metadata['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Real movie 1 fake\n",
    "mov_path = []\n",
    "real_list = metadata[metadata['label'] == 'REAL']['mov'].tolist()\n",
    "for path in real_list:\n",
    "    mov_path.append(metadata[metadata['original'] == path]['mov'].tolist()[0])\n",
    "mov_path.extend(real_list)\n",
    "mov_path = [os.path.join(data_dir, path) for path in mov_path]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Movie to Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_from_mov(video_file, show_img=False):\n",
    "    # https://note.nkmk.me/python-opencv-videocapture-file-camera/\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    image_list = []\n",
    "    for i in range(frames):\n",
    "        _, image = cap.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image_list.append(image)\n",
    "    cap.release()\n",
    "\n",
    "    if show_img:\n",
    "        fig, ax = plt.subplots(1,1, figsize=(15, 15))\n",
    "        ax.imshow(image[0])\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        ax.title.set_text(f\"FRAME 0: {video_file.split('/')[-1]}\")\n",
    "        plt.grid(False)\n",
    "        \n",
    "    return image_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(img):\n",
    "    # Add Dataset \"Haarcascades\"\n",
    "    face_cascade = cv2.CascadeClassifier('..\\\\haarcascade\\\\haarcascade_frontalface_alt.xml')\n",
    "    face_crops = face_cascade.detectMultiScale(img, scaleFactor=1.3, minNeighbors=5)\n",
    "    \n",
    "    if len(face_crops) == 0:\n",
    "        return []\n",
    "    \n",
    "    crop_imgs = []\n",
    "    for i in range(len(face_crops)):\n",
    "        x = face_crops[i][0]\n",
    "        y = face_crops[i][1]\n",
    "        w = face_crops[i][2]\n",
    "        h = face_crops[i][3]\n",
    "        #x,y,w,h=ratio*x,ratio*y,ratio*w,ratio*h\n",
    "        crop_imgs.append(img[y:y+h, x:x+w])\n",
    "    return crop_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Search Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Movie which cannot detect face\n",
    "def detect_noface_mov(mov_path, search_img_num=None):\n",
    "    no_face_detect_mov = []\n",
    "\n",
    "    for mov in tqdm(mov_path):\n",
    "        # VideoCapture\n",
    "        image = get_img_from_mov(mov)\n",
    "        flag = 0\n",
    "        \n",
    "        if search_img_num is None:\n",
    "            num = len(image)\n",
    "        else:\n",
    "            num = search_img_num + 1\n",
    "        \n",
    "        for idx in range(num):\n",
    "            crop = detect_face(image[idx])\n",
    "\n",
    "            if crop != []:\n",
    "                flag += 1\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        if flag == 0:\n",
    "            no_face_detect_mov.append(mov)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'id': np.arange(len(no_face_detect_mov)),\n",
    "        'mov': no_face_detect_mov\n",
    "    })\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_face_detect_mov_frame = detect_noface_mov(mov_path, search_img_num=0)\n",
    "# no_face_detect_mov_frame.to_csv('../input/no_face_mov_01.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(object):\n",
    "    def __init__(self, size=300):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image = cv2.resize(image, (self.size,\n",
    "                                 self.size))\n",
    "        return image\n",
    "\n",
    "# Data Augumentation\n",
    "class ImageTransform():\n",
    "    def __init__(self, resize):\n",
    "        self.data_transform = {\n",
    "            'train': transforms.Compose([\n",
    "                Resize(resize),\n",
    "                transforms.ToTensor(),\n",
    "            ]),\n",
    "            'val': transforms.Compose([\n",
    "                Resize(resize),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "        }\n",
    "        self.size = resize\n",
    "        \n",
    "    def __call__(self, img, phase):\n",
    "        return self.data_transform[phase](img)\n",
    "\n",
    "\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, file_list, metadata, transform=None, phase='train'):\n",
    "        self.file_list = file_list\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "        self.phase = phase\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        mov_path = self.file_list[idx]\n",
    "        \n",
    "        # Label\n",
    "        label = self.metadata[self.metadata['mov'] == mov_path.split('\\\\')[-1]]['label'].values\n",
    "        \n",
    "        if label == 'FAKE':\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        \n",
    "        # Movie to Image\n",
    "        for i in range(30):\n",
    "            try:\n",
    "                image = get_img_from_mov(mov_path, show_img=False)[i*10]\n",
    "                # FaceCrop\n",
    "                image = detect_face(image)[0]\n",
    "                # Transform\n",
    "                image = self.transform(image, self.phase)\n",
    "                # When the face is detected, exit from the loop\n",
    "                if torch.is_tensor(image):\n",
    "                    break\n",
    "            except:\n",
    "                image = None\n",
    "                pass\n",
    "            \n",
    "        # When the face cannot be detected...\n",
    "        if image is None:\n",
    "            image = torch.ones((3, self.transform.size, self.transform.size), dtype=torch.float)\n",
    "            label = 0.5\n",
    "        \n",
    "        return image, label, mov_path\n",
    "    \n",
    "    \n",
    "class DeepfakeDataset_idx0(Dataset):\n",
    "    def __init__(self, file_list, metadata, transform=None, phase='train'):\n",
    "        self.file_list = file_list\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "        self.phase = phase\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        mov_path = self.file_list[idx]\n",
    "        \n",
    "        # Label\n",
    "        label = self.metadata[self.metadata['mov'] == mov_path.split('\\\\')[-1]]['label'].values\n",
    "        \n",
    "        if label == 'FAKE':\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        \n",
    "        # Movie to Image\n",
    "        try:\n",
    "            image = get_img_from_mov(mov_path, show_img=False)[0]\n",
    "            # FaceCrop\n",
    "            image = detect_face(image)[0]\n",
    "            # Transform\n",
    "            image = self.transform(image, self.phase)\n",
    "        except:\n",
    "            image = torch.ones((3, self.transform.size, self.transform.size), dtype=torch.float)\n",
    "            label = 0.5\n",
    "        \n",
    "        return image, label, mov_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# VGG19\n",
    "use_pretrained = True\n",
    "net = models.vgg19(pretrained=use_pretrained)\n",
    "# Change Last Layer\n",
    "# Output Features 1000 → 2\n",
    "net.classifier[6] = nn.Linear(in_features=4096, out_features=2)\n",
    "\n",
    "# Specify The Layers for updating\n",
    "params_to_update = []\n",
    "update_params_name = ['classifier.6.weight', 'classifier.6.bias']\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    if name in update_params_name:\n",
    "        param.requires_grad = True\n",
    "        params_to_update.append(param)\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set RawData  #########################################################\n",
    "# Divide Train, Valid Dataset\n",
    "train_mov_path, val_mov_path = train_test_split(mov_path, test_size=0.1)\n",
    "\n",
    "# Config  #########################################################\n",
    "size = 224\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 32\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params=params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "# Dataset  #########################################################\n",
    "train_dataset = DeepfakeDataset_idx0(train_mov_path, metadata, ImageTransform(size), 'train')\n",
    "val_dataset = DeepfakeDataset_idx0(val_mov_path, metadata, ImageTransform(size), 'val')\n",
    "\n",
    "# Dataloader  #########################################################\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "dataloader_dict = {\n",
    "    'train': train_dataloader,\n",
    "    'val': val_dataloader\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, dataloader_dict, criterion, optimizer, num_epoch):\n",
    "    \n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(net.state_dict())\n",
    "    best_acc = 0.0\n",
    "    net = net.to(device)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epoch))\n",
    "        print('-'*20)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            \n",
    "            if phase == 'train':\n",
    "                net.train()\n",
    "            else:\n",
    "                net.eval()\n",
    "                \n",
    "            epoch_loss = 0.0\n",
    "            epoch_corrects = 0\n",
    "            \n",
    "            for inputs, labels, _ in tqdm(dataloader_dict[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device, dtype=torch.long)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                    epoch_loss += loss.item() * inputs.size(0)\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "                    \n",
    "                del inputs, labels\n",
    "                torch.cuda.empty_cache()\n",
    "                    \n",
    "            epoch_loss = epoch_loss / len(dataloader_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double() / len(dataloader_dict[phase].dataset)\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(net.state_dict())\n",
    "                \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    net.load_state_dict(best_model_wts)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\kaggle\\deepfa~1\\env\\venv~1\\lib\\site-packages\\ipykernel_launcher.py:22: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6274ab0ae50a4bc990c2355f8651d955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=37.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train\n",
    "num_epoch = 5\n",
    "net = train_model(net, dataloader_dict, criterion, optimizer, num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "date = datetime.datetime.now().strftime('%Y%m%d')\n",
    "torch.save(net.state_dict(), \"../model/vgg19_ep{}_{}.pth\".format(num_epoch, date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\kaggle\\deepfa~1\\env\\venv~1\\lib\\site-packages\\ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7300eed6a95942c98774620d99c25d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=90.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "label_list = []\n",
    "pred_list = []\n",
    "path_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, label, mov_path in tqdm(val_dataset):\n",
    "        img = img.unsqueeze(0)\n",
    "        img = img.to(device)\n",
    "\n",
    "        net.eval()\n",
    "\n",
    "        outputs = net(img)\n",
    "        preds = F.softmax(outputs, dim=1)[:, 1].tolist()\n",
    "        \n",
    "        label_list.append(label)\n",
    "        pred_list.append(preds[0])\n",
    "        path_list.append(mov_path)\n",
    "    \n",
    "res = pd.DataFrame({\n",
    "    'label': label_list,\n",
    "    'pred': pred_list,\n",
    "    'path': path_list\n",
    "})\n",
    "\n",
    "res.to_csv('../output/submission_vgg19.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.987737</td>\n",
       "      <td>..\\input\\iwhrsvsqcs.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.996737</td>\n",
       "      <td>..\\input\\yuozmggtin.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.995407</td>\n",
       "      <td>..\\input\\doiljuhamq.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.905448</td>\n",
       "      <td>..\\input\\owmzjclufi.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.990909</td>\n",
       "      <td>..\\input\\okrrtlhqlz.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1</td>\n",
       "      <td>0.997594</td>\n",
       "      <td>..\\input\\enpnclhmik.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>0.997899</td>\n",
       "      <td>..\\input\\dwardtgkwh.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1</td>\n",
       "      <td>0.985861</td>\n",
       "      <td>..\\input\\uqfnqniamo.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>0.974265</td>\n",
       "      <td>..\\input\\hcanfkwivl.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1</td>\n",
       "      <td>0.934587</td>\n",
       "      <td>..\\input\\flndbxfdta.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label      pred                     path\n",
       "0       1  0.987737  ..\\input\\iwhrsvsqcs.mp4\n",
       "1       1  0.996737  ..\\input\\yuozmggtin.mp4\n",
       "2       1  0.995407  ..\\input\\doiljuhamq.mp4\n",
       "3       1  0.905448  ..\\input\\owmzjclufi.mp4\n",
       "4       1  0.990909  ..\\input\\okrrtlhqlz.mp4\n",
       "..    ...       ...                      ...\n",
       "85      1  0.997594  ..\\input\\enpnclhmik.mp4\n",
       "86      1  0.997899  ..\\input\\dwardtgkwh.mp4\n",
       "87      1  0.985861  ..\\input\\uqfnqniamo.mp4\n",
       "88      1  0.974265  ..\\input\\hcanfkwivl.mp4\n",
       "89      1  0.934587  ..\\input\\flndbxfdta.mp4\n",
       "\n",
       "[90 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ee8b8f8548>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.scatterplot(x='label', y='pred', data=res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
