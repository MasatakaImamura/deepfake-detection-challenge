{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V12: Fix Data Augmentation (Resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# https://www.kaggle.com/timesler/facial-recognition-model-in-pytorch\n",
    "# Install facenet-pytorch\n",
    "!pip install /kaggle/input/facenet-pytorch-vggface2/facenet_pytorch-1.0.1-py3-none-any.whl\n",
    "# Copy model checkpoints to torch cache so they are loaded automatically by the package\n",
    "!mkdir -p /tmp/.cache/torch/checkpoints/\n",
    "!cp /kaggle/input/facenet-pytorch-vggface2/20180402-114759-vggface2-logits.pth /tmp/.cache/torch/checkpoints/vggface2_DG3kwML46X.pt\n",
    "!cp /kaggle/input/facenet-pytorch-vggface2/20180402-114759-vggface2-features.pth /tmp/.cache/torch/checkpoints/vggface2_G5aNV2VSMn.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "package_path = '../input/efficientnet-pytorch/EfficientNet-PyTorch/EfficientNet-PyTorch-master'\n",
    "sys.path.append(package_path)\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['efficientnet_b0_10_epoch_44_loss_0.121.pth',\n",
       " 'efficientnet_b0_5_epoch_28_loss_0.125.pth',\n",
       " 'efficientnet_b0_6_epoch_28_loss_0.136.pth',\n",
       " 'efficientnet_b0_7_epoch_23_loss_0.128.pth',\n",
       " 'efficientnet_b4_2_epoch_42_loss_0.105.pth',\n",
       " 'efficientnet_b4_3_epoch_44_loss_0.113.pth',\n",
       " 'efficientnet_b4_4_epoch_5_loss_0.100.pth',\n",
       " 'efficientnet_b4_5_epoch_10_loss_0.116.pth',\n",
       " 'efficientnet_b4_5_epoch_17_loss_0.112.pth',\n",
       " 'efficientnet_b4_6_epoch_19_loss_0.180.pth',\n",
       " 'efficientnet_b6_2_epoch_3_loss_0.120.pth',\n",
       " 'efficientnet_lstm_b0_3_epoch_6_loss_0.098.pth',\n",
       " 'efficientnet_lstm_b0_4_epoch_6_loss_0.102.pth']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(os.listdir('../input/deepfake-efficientnet2d-model-weight-2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Trained Weight Path\n",
    "weight_path = 'efficientnet_b4_6_epoch_19_loss_0.180.pth'\n",
    "trained_weights_path = os.path.join('../input/deepfake-efficientnet2d-model-weight-2', weight_path)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ywauoonmlr.mp4',\n",
       " 'sfsayjgzrh.mp4',\n",
       " 'sngjsueuhs.mp4',\n",
       " 'eryjktdexi.mp4',\n",
       " 'nwvloufjty.mp4']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dir = '../input/deepfake-detection-challenge/test_videos'\n",
    "os.listdir(test_dir)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_from_mov(video_file, num_img, frame_window):\n",
    "    # https://note.nkmk.me/python-opencv-videocapture-file-camera/\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    image_list = []\n",
    "    for i in range(num_img):\n",
    "        _, image = cap.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image_list.append(image)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, (i + 1) * frame_window)\n",
    "        if cap.get(cv2.CAP_PROP_POS_FRAMES) >= frames:\n",
    "            break\n",
    "    cap.release()\n",
    "\n",
    "    return image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransform:\n",
    "    def __init__(self, size, mean, std):\n",
    "        self.data_transform = transforms.Compose([\n",
    "                transforms.Resize((size, size), interpolation=Image.BILINEAR),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.data_transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, file_list, device, detector, transform, img_num=20, frame_window=10):\n",
    "        self.file_list = file_list\n",
    "        self.device = device\n",
    "        self.detector = detector\n",
    "        self.transform = transform\n",
    "        self.img_num = img_num\n",
    "        self.frame_window = frame_window\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        mov_path = self.file_list[idx]\n",
    "        img_list = []\n",
    "\n",
    "        # Movie to Image\n",
    "        try:\n",
    "            all_image = get_img_from_mov(mov_path, self.img_num, self.frame_window)\n",
    "        except:\n",
    "            return [], mov_path.split('/')[-1]\n",
    "        \n",
    "        # Detect Faces\n",
    "        for image in all_image:\n",
    "            \n",
    "            try:\n",
    "                _image = image[np.newaxis, :, :, :]\n",
    "                boxes, probs = self.detector.detect(_image, landmarks=False)\n",
    "                x = int(boxes[0][0][0])\n",
    "                y = int(boxes[0][0][1])\n",
    "                z = int(boxes[0][0][2])\n",
    "                w = int(boxes[0][0][3])\n",
    "                image = image[y-15:w+15, x-15:z+15]\n",
    "                \n",
    "                # Preprocessing\n",
    "                image = Image.fromarray(image)\n",
    "                image = self.transform(image)\n",
    "                \n",
    "                img_list.append(image)\n",
    "\n",
    "            except:\n",
    "                img_list.append(None)\n",
    "            \n",
    "        # Padding None\n",
    "        img_list = [c for c in img_list if c is not None]\n",
    "        \n",
    "        return img_list, mov_path.split('/')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centerloss Version\n",
    "# class Efficientnet_centerloss(nn.Module):\n",
    "\n",
    "#     def __init__(self, output_size, model_name='efficientnet-b0'):\n",
    "#         super(Efficientnet_centerloss, self).__init__()\n",
    "\n",
    "#         self.base = EfficientNet.from_name(model_name)\n",
    "#         self.base._fc = nn.Linear(in_features=self.base._fc.in_features, out_features=512)\n",
    "#         self.model_name = model_name\n",
    "\n",
    "#         self.global_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "#         self.fc_1 = nn.Linear(512, 64, bias=True)\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "#         self.ip1 = nn.Linear(64, 2)\n",
    "#         self.fc_last = nn.Linear(2, output_size, bias=True)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.base(x)\n",
    "\n",
    "#         x = self.fc_1(x)\n",
    "#         x = self.dropout(x)\n",
    "#         ip1 = self.ip1(x)\n",
    "#         x = self.fc_last(ip1)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Efficientnet_centerloss(output_size=1, model_name='efficientnet-b0')\n",
    "# model.load_state_dict(torch.load(trained_weights_path, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original Version\n",
    "model = EfficientNet.from_name('efficientnet-b4')\n",
    "model._fc = nn.Linear(in_features=model._fc.in_features, out_features=1)\n",
    "model.load_state_dict(torch.load(trained_weights_path, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = [os.path.join(test_dir, path) for path in os.listdir(test_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/deepfake-detection-challenge/test_videos/ywauoonmlr.mp4',\n",
       " '../input/deepfake-detection-challenge/test_videos/sfsayjgzrh.mp4',\n",
       " '../input/deepfake-detection-challenge/test_videos/sngjsueuhs.mp4',\n",
       " '../input/deepfake-detection-challenge/test_videos/eryjktdexi.mp4',\n",
       " '../input/deepfake-detection-challenge/test_videos/nwvloufjty.mp4']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "def predict_dfdc(dataset, model):\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    pred_list = []\n",
    "    path_list = []\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(dataset))):\n",
    "            pred = 0\n",
    "            imgs, mov_path = dataset.__getitem__(i)\n",
    "            \n",
    "            # No get Image\n",
    "            if len(imgs) == 0:\n",
    "                pred_list.append(0.5)\n",
    "                path_list.append(mov_path)\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            for i in range(len(imgs)):\n",
    "                img = imgs[i]\n",
    "                \n",
    "                output = model(img.unsqueeze(0).to(device))\n",
    "                pred += torch.sigmoid(output).item() / len(imgs)\n",
    "                \n",
    "            pred_list.append(pred)\n",
    "            path_list.append(mov_path)\n",
    "            \n",
    "    torch.cuda.empty_cache()\n",
    "            \n",
    "    return path_list, pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6b7570a10a49c5ac8e07a141314dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=400), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "img_size = 120\n",
    "img_num = 15\n",
    "frame_window = 5\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "transform = ImageTransform(img_size, mean, std)\n",
    "\n",
    "detector = MTCNN(image_size=img_size, margin=14, keep_all=False, factor=0.5, \n",
    "                 select_largest=False, post_process=False, device=device).eval()\n",
    "\n",
    "dataset = DeepfakeDataset(test_file, device, detector, transform, img_num, frame_window)\n",
    "\n",
    "path_list, pred_list = predict_dfdc(dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission\n",
    "res = pd.DataFrame({\n",
    "    'filename': path_list,\n",
    "    'label': pred_list,\n",
    "})\n",
    "\n",
    "res.sort_values(by='filename', ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADQ9JREFUeJzt3X+MZWddx/H3h64VQbClO5DaotMmRdmQGMiEFElQKTFATbd/FFMiupqNG1ARxURW+QOj/7RGKZoQcEPR1SAUK7EbihosbVBiV6e0UtoVW8taVlY6RFp/RaHh6x/3QDbtbu+ZmXvvzHz3/Uo2c8+5z9n7ffbe+ewzzznnmVQVkqSd72lbXYAkaTYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCYMdElqwkCXpCZ2LfLFdu/eXcvLy4t8SUna8e66664vV9XStHYLDfTl5WVWV1cX+ZKStOMl+Zcx7ZxykaQmDHRJasJAl6QmDHRJasJAl6QmDHRJasJAl6QmDHRJasJAl6QmFnqnqCTtVMsHb93wscevu3KGlZyZI3RJasJAl6QmDHRJasJAl6QmDHRJasJAl6QmDHRJasJAl6QmDHRJasJAl6QmDHRJasJAl6QmDHRJasJAl6QmDHRJasJAl6QmDHRJasJAl6QmDHRJasJAl6QmDHRJasJAl6QmDHRJasJAl6QmDHRJasJAl6QmDHRJasJAl6QmRgV6kl9Mcl+Szyb5YJKnJ7kkydEkDyS5Kcm58y5WknRmUwM9yUXAzwMrVfUi4BzgWuB64Iaqugz4CrB/noVKkp7a2CmXXcC3JdkFPAM4CbwSuHl4/jBw9ezLkySNNTXQq+pfgd8CHmYS5I8BdwGPVtXjQ7MTwEWnOz7JgSSrSVbX1tZmU7Uk6UnGTLmcD+wFLgG+E3gm8JrTNK3THV9Vh6pqpapWlpaWNlOrJOkpjJlyeRXw+apaq6qvAR8Bvh84b5iCAbgY+OKcapQkjTAm0B8GLk/yjCQBrgDuB24Hrhna7ANumU+JkqQxxsyhH2Vy8vPTwL3DMYeAtwFvTfIgcAFw4xzrlCRNsWt6E6iqdwDveMLuh4CXzrwiSdKGeKeoJDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSE6OWz90Olg/euuFjj1935QwrkaTtyRG6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDVhoEtSEwa6JDUxKtCTnJfk5iT/mORYkpcleU6Sjyd5YPh6/ryLlSSd2dgR+u8Af1FV3wt8H3AMOAjcVlWXAbcN25KkLTI10JM8G3gFcCNAVX21qh4F9gKHh2aHgavnVaQkaboxI/RLgTXg95PcneR9SZ4JPK+qTgIMX587xzolSVOMCfRdwEuA91TVi4H/Zh3TK0kOJFlNsrq2trbBMiVJ04wJ9BPAiao6OmzfzCTgv5TkQoDh6yOnO7iqDlXVSlWtLC0tzaJmSdJpTA30qvo34AtJvmfYdQVwP3AE2Dfs2wfcMpcKJUmj7BrZ7s3AB5KcCzwE/BST/ww+nGQ/8DDwuvmUKEkaY1SgV9U9wMppnrpituVIkjbKO0UlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqYnRgZ7knCR3J/nosH1JkqNJHkhyU5Jz51emJGmaXeto+xbgGPDsYft64Iaq+lCS9wL7gffMuL6ZWD5464aPPX7dlTOsRJLmZ9QIPcnFwJXA+4btAK8Ebh6aHAaunkeBkqRxxk65vAv4ZeDrw/YFwKNV9fiwfQK4aMa1SZLWYWqgJ/kR4JGquuvU3adpWmc4/kCS1SSra2trGyxTkjTNmBH6y4GrkhwHPsRkquVdwHlJvjEHfzHwxdMdXFWHqmqlqlaWlpZmULIk6XSmBnpV/UpVXVxVy8C1wCeq6seA24Frhmb7gFvmVqUkaarNXIf+NuCtSR5kMqd+42xKkiRtxHouW6Sq7gDuGB4/BLx09iVJkjbCO0UlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaMNAlqQkDXZKaWNeNRZK0k23mdyPsBI7QJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJakJA12SmjDQJamJXVtdgCStx/LBW7e6hG1r6gg9yfOT3J7kWJL7krxl2P+cJB9P8sDw9fz5lytJOpMxUy6PA79UVS8ELgd+Nske4CBwW1VdBtw2bEuStsjUQK+qk1X16eHxfwLHgIuAvcDhodlh4Op5FSlJmm5dJ0WTLAMvBo4Cz6uqkzAJfeC5ZzjmQJLVJKtra2ubq1aSdEajAz3JtwN/CvxCVf3H2OOq6lBVrVTVytLS0kZqlCSNMCrQk3wLkzD/QFV9ZNj9pSQXDs9fCDwynxIlSWOMucolwI3Asap65ylPHQH2DY/3AbfMvjxJ0lhjrkN/OfDjwL1J7hn2/SpwHfDhJPuBh4HXzadESdIYUwO9qv4GyBmevmK25UiSNspb/yWpCQNdkpow0CWpCQNdkpow0CWpCZfPlbRum13C9vh1V86oEp3KEbokNWGgS1ITBrokNeEc+hRb+euunGeUtB6O0CWpCQNdkpow0CWpCQNdkprwpGhTmzmZ68lYaWdyhC5JTThCl3awnfqT2FZeDtyZI3RJasIR+jbmKObs4PusWXGELklNOELXk+zUpVG3aj7ZEba2C0foktSEgS5JTTjlopnbqZfSSTudI3RJasIRusTZeWLzbOxzd47QJakJA12SmjDQJakJ59C1rTivK22cI3RJasJAl6QmDHRJasJAl6QmNhXoSV6d5HNJHkxycFZFSZLWb8OBnuQc4N3Aa4A9wOuT7JlVYZKk9dnMCP2lwINV9VBVfRX4ELB3NmVJktZrM4F+EfCFU7ZPDPskSVtgMzcW5TT76kmNkgPAgWHzv5J8bp2vsxv48jqP6cB+n13sd2O5/km71tvv7x7TaDOBfgJ4/inbFwNffGKjqjoEHNroiyRZraqVjR6/U9nvs4v9PrvMq9+bmXL5e+CyJJckORe4Fjgym7IkSeu14RF6VT2e5OeAvwTOAd5fVffNrDJJ0rpsanGuqvoY8LEZ1XImG56u2eHs99nFfp9d5tLvVD3pPKYkaQfy1n9JamLbBPq0ZQSSfGuSm4bnjyZZXnyVszei329Ncn+SzyS5Lcmoy5e2u7HLRiS5JkklaXElxJh+J/nR4T2/L8kfL7rGWRvxGf+uJLcnuXv4nL92K+qctSTvT/JIks+e4fkk+d3h3+UzSV6y6Retqi3/w+Sk6j8DlwLnAv8A7HlCm58B3js8vha4aavrXlC/fwh4xvD4TWdLv4d2zwI+CdwJrGx13Qt6vy8D7gbOH7afu9V1L6DPh4A3DY/3AMe3uu4Z9f0VwEuAz57h+dcCf87knp7LgaObfc3tMkIfs4zAXuDw8Phm4Iokp7u5aSeZ2u+qur2q/mfYvJPJ9f473dhlI34D+E3gfxdZ3ByN6fdPA++uqq8AVNUjC65x1sb0uYBnD4+/g9Pcz7ITVdUngX9/iiZ7gT+siTuB85JcuJnX3C6BPmYZgW+2qarHgceACxZS3fysd/mE/Uz+R9/ppvY7yYuB51fVRxdZ2JyNeb9fALwgyaeS3Jnk1Qurbj7G9PnXgDckOcHkqrk3L6a0LTfz5VO2y+8UHbOMwKilBnaY0X1K8gZgBfiBuVa0GE/Z7yRPA24AfnJRBS3ImPd7F5Nplx9k8tPYXyd5UVU9Oufa5mVMn18P/EFV/XaSlwF/NPT56/Mvb0vNPNO2ywh9zDIC32yTZBeTH82e6seZnWDU8glJXgW8Hbiqqv5vQbXN07R+Pwt4EXBHkuNM5hePNDgxOvZzfktVfa2qPg98jknA71Rj+rwf+DBAVf0t8HQma510N+r7fz22S6CPWUbgCLBveHwN8IkazizsYFP7PUw9/B6TMN/p86nf8JT9rqrHqmp3VS1X1TKTcwdXVdXq1pQ7M2M+53/G5EQ4SXYzmYJ5aKFVztaYPj8MXAGQ5IVMAn1toVVujSPATwxXu1wOPFZVJzf1N271meAnnPH9JyZnxN8+7Pt1Jt/IMHmT/wR4EPg74NKtrnlB/f4r4EvAPcOfI1td8yL6/YS2d9DgKpeR73eAdwL3A/cC1251zQvo8x7gU0yugLkH+OGtrnlG/f4gcBL4GpPR+H7gjcAbT3mv3z38u9w7i8+4d4pKUhPbZcpFkrRJBrokNWGgS1ITBrokNWGgS1ITBrokNWGgS1ITBrokNfH/17JLBYRa/d0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(res['label'], 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>aassnaulhq.mp4</td>\n",
       "      <td>0.899037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>aayfryxljh.mp4</td>\n",
       "      <td>0.012424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>acazlolrpz.mp4</td>\n",
       "      <td>0.891982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>adohdulfwb.mp4</td>\n",
       "      <td>0.007394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>ahjnxtiamx.mp4</td>\n",
       "      <td>0.738787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>ajiyrjfyzp.mp4</td>\n",
       "      <td>0.645911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>aktnlyqpah.mp4</td>\n",
       "      <td>0.961957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>alrtntfxtd.mp4</td>\n",
       "      <td>0.912442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>aomqqjipcp.mp4</td>\n",
       "      <td>0.963147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>apedduehoy.mp4</td>\n",
       "      <td>0.039599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename     label\n",
       "374  aassnaulhq.mp4  0.899037\n",
       "247  aayfryxljh.mp4  0.012424\n",
       "339  acazlolrpz.mp4  0.891982\n",
       "193  adohdulfwb.mp4  0.007394\n",
       "315  ahjnxtiamx.mp4  0.738787\n",
       "248  ajiyrjfyzp.mp4  0.645911\n",
       "367  aktnlyqpah.mp4  0.961957\n",
       "340  alrtntfxtd.mp4  0.912442\n",
       "292  aomqqjipcp.mp4  0.963147\n",
       "348  apedduehoy.mp4  0.039599"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Post Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_res = res.copy()\n",
    "\n",
    "x_min = _res['label'].min()\n",
    "x_max = _res['label'].max()\n",
    "\n",
    "set_min = 0.1\n",
    "set_max = 0.9\n",
    "\n",
    "def postprocess(x):\n",
    "    return (x - x_min) * (set_max - set_min) / (x_max - x_min) + set_min\n",
    "\n",
    "_res['label'] = res['label'].apply(postprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(_res['label'], 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>aassnaulhq.mp4</td>\n",
       "      <td>0.826302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>aayfryxljh.mp4</td>\n",
       "      <td>0.105597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>acazlolrpz.mp4</td>\n",
       "      <td>0.820567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>adohdulfwb.mp4</td>\n",
       "      <td>0.101508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>ahjnxtiamx.mp4</td>\n",
       "      <td>0.696038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>ajiyrjfyzp.mp4</td>\n",
       "      <td>0.620541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>aktnlyqpah.mp4</td>\n",
       "      <td>0.877447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>alrtntfxtd.mp4</td>\n",
       "      <td>0.837198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>aomqqjipcp.mp4</td>\n",
       "      <td>0.878415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>apedduehoy.mp4</td>\n",
       "      <td>0.127687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           filename     label\n",
       "374  aassnaulhq.mp4  0.826302\n",
       "247  aayfryxljh.mp4  0.105597\n",
       "339  acazlolrpz.mp4  0.820567\n",
       "193  adohdulfwb.mp4  0.101508\n",
       "315  ahjnxtiamx.mp4  0.696038\n",
       "248  ajiyrjfyzp.mp4  0.620541\n",
       "367  aktnlyqpah.mp4  0.877447\n",
       "340  alrtntfxtd.mp4  0.837198\n",
       "292  aomqqjipcp.mp4  0.878415\n",
       "348  apedduehoy.mp4  0.127687"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_res.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _res.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "662b14fd15454140a5ee28800d27700c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6f00c5e994c84533944d48acd1fee5e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8ba5c999fa0143b0b0a64f0a64fa379f",
       "placeholder": "​",
       "style": "IPY_MODEL_662b14fd15454140a5ee28800d27700c",
       "value": " 400/400 [48:06&lt;00:00,  7.22s/it]"
      }
     },
     "858758037ad14a13858d3715ca0a8fb0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "8ba5c999fa0143b0b0a64f0a64fa379f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8c1eadd91f5c43569e8772a68a84249c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9c61c5f4f0884e4081500d3647e37832": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a4c2fbea5d4045129f7999515268441a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8c1eadd91f5c43569e8772a68a84249c",
       "max": 400,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_858758037ad14a13858d3715ca0a8fb0",
       "value": 400
      }
     },
     "bf6b7570a10a49c5ac8e07a141314dcd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a4c2fbea5d4045129f7999515268441a",
        "IPY_MODEL_6f00c5e994c84533944d48acd1fee5e9"
       ],
       "layout": "IPY_MODEL_9c61c5f4f0884e4081500d3647e37832"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
