{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data_dir = '..\\\\input'\n",
    "\n",
    "metadata_path = glob.glob(os.path.join(data_dir, '*.json'))\n",
    "mov_path = glob.glob(os.path.join(data_dir, '*.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata\n",
    "metadata = pd.DataFrame()\n",
    "for path in metadata_path:\n",
    "    metadata = pd.concat([metadata, pd.read_json(path).T], axis=0)\n",
    "    \n",
    "metadata.reset_index(inplace=True)\n",
    "metadata = metadata.rename(columns={'index': 'mov'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mov</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>owxbbpjpch.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>wynotylpnm.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vpmyeepbep.mp4</td>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fzvpbrzssi.mp4</td>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>htorvhbcae.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>wclvkepakb.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fckxaqjbxk.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>vpmyeepbep.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mov label  split        original\n",
       "0  owxbbpjpch.mp4  FAKE  train  wynotylpnm.mp4\n",
       "1  vpmyeepbep.mp4  REAL  train             NaN\n",
       "2  fzvpbrzssi.mp4  REAL  train             NaN\n",
       "3  htorvhbcae.mp4  FAKE  train  wclvkepakb.mp4\n",
       "4  fckxaqjbxk.mp4  FAKE  train  vpmyeepbep.mp4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FAKE    2839\n",
       "REAL     194\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label\n",
    "metadata['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Movie to Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_from_mov(video_file, show_img=False):\n",
    "    # https://note.nkmk.me/python-opencv-videocapture-file-camera/\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    image_list = []\n",
    "    for i in range(frames):\n",
    "        _, image = cap.read()\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image_list.append(image)\n",
    "    cap.release()\n",
    "\n",
    "    if show_img:\n",
    "        fig, ax = plt.subplots(1,1, figsize=(15, 15))\n",
    "        ax.imshow(image[0])\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        ax.title.set_text(f\"FRAME 0: {video_file.split('/')[-1]}\")\n",
    "        plt.grid(False)\n",
    "        \n",
    "    return image_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(img):\n",
    "    # Add Dataset \"Haarcascades\"\n",
    "    face_cascade = cv2.CascadeClassifier('..\\\\haarcascade\\\\haarcascade_frontalface_alt.xml')\n",
    "    face_crops = face_cascade.detectMultiScale(img, scaleFactor=1.3, minNeighbors=5)\n",
    "    \n",
    "    if len(face_crops) == 0:\n",
    "        return []\n",
    "    \n",
    "    crop_imgs = []\n",
    "    for i in range(len(face_crops)):\n",
    "        x = face_crops[i][0]\n",
    "        y = face_crops[i][1]\n",
    "        w = face_crops[i][2]\n",
    "        h = face_crops[i][3]\n",
    "        #x,y,w,h=ratio*x,ratio*y,ratio*w,ratio*h\n",
    "        crop_imgs.append(img[y:y+h, x:x+w])\n",
    "    return crop_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Search Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Movie which cannot detect face\n",
    "def detect_noface_mov(mov_path, search_img_num=None):\n",
    "    no_face_detect_mov = []\n",
    "\n",
    "    for mov in tqdm(mov_path):\n",
    "        # VideoCapture\n",
    "        image = get_img_from_mov(mov)\n",
    "        flag = 0\n",
    "        \n",
    "        if search_img_num is None:\n",
    "            num = len(image)\n",
    "        else:\n",
    "            num = search_img_num + 1\n",
    "        \n",
    "        for idx in range(num):\n",
    "            crop = detect_face(image[idx])\n",
    "\n",
    "            if crop != []:\n",
    "                flag += 1\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        if flag == 0:\n",
    "            no_face_detect_mov.append(mov)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'id': np.arange(len(no_face_detect_mov)),\n",
    "        'mov': no_face_detect_mov\n",
    "    })\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_face_detect_mov_frame = detect_noface_mov(mov_path, search_img_num=0)\n",
    "# no_face_detect_mov_frame.to_csv('../input/no_face_mov_01.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(object):\n",
    "    def __init__(self, size=300):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image = cv2.resize(image, (self.size,\n",
    "                                 self.size))\n",
    "        return image\n",
    "\n",
    "# Data Augumentation\n",
    "class ImageTransform():\n",
    "    def __init__(self, resize):\n",
    "        self.data_transform = {\n",
    "            'train': transforms.Compose([\n",
    "                Resize(resize),\n",
    "                transforms.ToTensor(),\n",
    "            ]),\n",
    "            'val': transforms.Compose([\n",
    "                Resize(resize),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "    def __call__(self, img, phase):\n",
    "        return self.data_transform[phase](img)\n",
    "\n",
    "\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, file_list, metadata, transform=None, phase='train'):\n",
    "        self.file_list = file_list\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "        self.phase = phase\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        mov_path = self.file_list[idx]\n",
    "        # first frame image only\n",
    "        image = get_img_from_mov(mov_path, show_img=False)[0]\n",
    "        # FaceCrop\n",
    "        image = detect_face(image)[0]\n",
    "        # Transform\n",
    "        image = self.transform(image, self.phase)\n",
    "        \n",
    "        # Label\n",
    "        label = self.metadata[self.metadata['mov'] == mov_path.split('\\\\')[-1]]['label'].values\n",
    "        \n",
    "        if label == 'FAKE':\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        \n",
    "        return image, label, mov_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# VGG16\n",
    "use_pretrained = True\n",
    "net = models.vgg16(pretrained=use_pretrained)\n",
    "# Change Last Layer\n",
    "# Output Features 1000 → 2\n",
    "net.classifier[6] = nn.Linear(in_features=4096, out_features=2)\n",
    "\n",
    "# Specify The Layers for updating\n",
    "params_to_update = []\n",
    "update_params_name = ['classifier.6.weight', 'classifier.6.bias']\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    if name in update_params_name:\n",
    "        param.requires_grad = True\n",
    "        params_to_update.append(param)\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set RawData  #########################################################\n",
    "# All Mp4 List\n",
    "mov_path = glob.glob(os.path.join(data_dir, '*.mp4'))\n",
    "\n",
    "# No Face Mov List\n",
    "no_face_list = pd.read_csv('../input/no_face_mov_01.csv')\n",
    "\n",
    "# Only Face Detected Movie List\n",
    "mov_path = [path for path in mov_path if path.split('\\\\')[-1] not in no_face_list['mov'].tolist()]\n",
    "len(mov_path)\n",
    "\n",
    "# Divide Train, Valid Dataset\n",
    "train_mov_path, val_mov_path = train_test_split(mov_path, test_size=0.1)\n",
    "\n",
    "# Config  #########################################################\n",
    "size = 224\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params=params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "# Dataset  #########################################################\n",
    "train_dataset = DeepfakeDataset(train_mov_path, metadata, ImageTransform(size), 'train')\n",
    "val_dataset = DeepfakeDataset(val_mov_path, metadata, ImageTransform(size), 'val')\n",
    "\n",
    "# Dataloader  #########################################################\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "dataloader_dict = {\n",
    "    'train': train_dataloader,\n",
    "    'val': val_dataloader\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, dataloader_dict, criterion, optimizer, num_epoch):\n",
    "    \n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(net.state_dict())\n",
    "    best_acc = 0.0\n",
    "    net = net.to(device)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epoch))\n",
    "        print('-'*20)\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            \n",
    "            if phase == 'train':\n",
    "                net.train()\n",
    "            else:\n",
    "                net.eval()\n",
    "                \n",
    "            epoch_loss = 0.0\n",
    "            epoch_corrects = 0\n",
    "            \n",
    "            for inputs, labels, _ in tqdm(dataloader_dict[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = net(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                    epoch_loss += loss.item() * inputs.size(0)\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "                    \n",
    "            epoch_loss = epoch_loss / len(dataloader_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double() / len(dataloader_dict[phase].dataset)\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(net.state_dict())\n",
    "                \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    net.load_state_dict(best_model_wts)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\kaggle\\deepfa~1\\env\\venv~1\\lib\\site-packages\\ipykernel_launcher.py:22: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6eeab247f614715bec8aa91f5c7b6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=202.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.4655 Acc: 0.8758\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017fe74bac574b4eb6a0037fac9bbaa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.3766 Acc: 0.9000\n",
      "Epoch 2/7\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f6ea66cdf845f1818580dc924d63e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=202.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train Loss: 0.5152 Acc: 0.8770\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87a4d294a81458eaa7c84039905fac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=23.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "val Loss: 0.3286 Acc: 0.9222\n",
      "Epoch 3/7\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275e93ca2aa54d4486d2af016b5c5e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=202.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train\n",
    "num_epoch = 7\n",
    "net = train_model(net, dataloader_dict, criterion, optimizer, num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model\n",
    "date = datetime.datetime.now().strftime('%Y%m%d')\n",
    "torch.save(net.state_dict(), \"../model/vgg16_ep{}_{}.pth\".format(num_epoch, date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\kaggle\\deepfa~1\\env\\venv~1\\lib\\site-packages\\ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7dc1b80d58544208b2f52127e4a74c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=90.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "label_list = []\n",
    "pred_list = []\n",
    "path_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, label, mov_path in tqdm(val_dataset):\n",
    "        img = img.unsqueeze(0)\n",
    "        img = img.to(device)\n",
    "\n",
    "        net.eval()\n",
    "\n",
    "        outputs = net(img)\n",
    "        preds = F.softmax(outputs, dim=1)[:, 1].tolist()\n",
    "        \n",
    "        label_list.append(label)\n",
    "        pred_list.append(preds[0])\n",
    "        path_list.append(mov_path)\n",
    "    \n",
    "res = pd.DataFrame({\n",
    "    'label': label_list,\n",
    "    'pred': pred_list,\n",
    "    'path': path_list\n",
    "})\n",
    "\n",
    "res.to_csv('../output/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.938446</td>\n",
       "      <td>..\\input\\uzcpzcdwfv.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.795264</td>\n",
       "      <td>..\\input\\nswtvttxre.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.763813</td>\n",
       "      <td>..\\input\\lqrfjiteca.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.943662</td>\n",
       "      <td>..\\input\\pbadlmyohy.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.567117</td>\n",
       "      <td>..\\input\\getmctvhmd.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1</td>\n",
       "      <td>0.927722</td>\n",
       "      <td>..\\input\\bnitzihcqs.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>0.948045</td>\n",
       "      <td>..\\input\\yuteraqwck.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1</td>\n",
       "      <td>0.880322</td>\n",
       "      <td>..\\input\\hfedtplyys.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>0.619810</td>\n",
       "      <td>..\\input\\cnbxddelwb.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1</td>\n",
       "      <td>0.188667</td>\n",
       "      <td>..\\input\\gecrjewspu.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label      pred                     path\n",
       "0       1  0.938446  ..\\input\\uzcpzcdwfv.mp4\n",
       "1       0  0.795264  ..\\input\\nswtvttxre.mp4\n",
       "2       1  0.763813  ..\\input\\lqrfjiteca.mp4\n",
       "3       1  0.943662  ..\\input\\pbadlmyohy.mp4\n",
       "4       1  0.567117  ..\\input\\getmctvhmd.mp4\n",
       "..    ...       ...                      ...\n",
       "85      1  0.927722  ..\\input\\bnitzihcqs.mp4\n",
       "86      1  0.948045  ..\\input\\yuteraqwck.mp4\n",
       "87      1  0.880322  ..\\input\\hfedtplyys.mp4\n",
       "88      1  0.619810  ..\\input\\cnbxddelwb.mp4\n",
       "89      1  0.188667  ..\\input\\gecrjewspu.mp4\n",
       "\n",
       "[90 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2323071b908>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.scatterplot(x='label', y='pred', data=res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.to('cpu'), '../output/vgg16_ep{}_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
